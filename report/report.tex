\documentclass[a4paper]{article}

\usepackage[english]{babel}  % Language support

%% Font selection
\usepackage{newpxtext,newpxmath}
\usepackage[T1]{fontenc}
\usepackage{microtype}

%% Listings (see https://www.ctan.org/pkg/minted for installation)
\usepackage[outputdir=.texpadtmp]{minted}
\setminted{	python3 = true,
			fontsize = \small, 
			frame = leftline, 
			framesep = 2\fboxsep}
\usepackage{upquote}  % TT quotes in listings

%% Graphics
\usepackage{graphicx, epstopdf}  % Support EPS figures
\epstopdfsetup{outdir=.texpadtmp/}

%% Bibliography
\usepackage[style=ieee, isbn=false]{biblatex}
\addbibresource{bibliography.bib}

%% Other packages
\usepackage{framed}  % Enables frames around quote env.
\usepackage{url, hyperref}  % \url command, properly working

%% Shortcuts
\usepackage{xspace}
\newcommand{\matlab}{\textsc{Matlab}\xspace}
\newcommand{\csv}{\texttt{CSV}\xspace}
\newcommand{\newpaper}[2]{\subsection*{\cite{#1}. \citetitle{#1}}

\begin{framed}\begin{quote}\begin{description}
	\item [Authors]\citeauthor{#1}
	\item [Title]\citetitle{#1}
	\item [Year]\citedate{#1}
	\item [Keywords]\texttt{#2}
\end{description}\end{quote}\end{framed}

}

%% Last package to be loaded (for compatibility issues)
\usepackage{cleveref}  % Allows en­hanced cross-ref­er­enc­ing fea­tures

%% Document details
\title{Piecewise analysis of stock market data}
\author{Paolo Antonini \and Davide Azzalini \and Fabio Azzalini}
\date{}

\usepackage{lipsum}  %TODO REV Remove

%% DOCUMENT
\begin{document}

\maketitle

\begin{abstract}
Time series data is characterised as large in size, high dimensionality and continuous updates. Moreover, it is always considered as a whole, instead of individual numerical fields. As a consequence, in order to analyse and mine time series data, segmentation and dimensionality reduction are essential, as well as trend identification. In particular, in the following pages we are going to collect and summarise some segmentation methods, applied in particular to stock market data. As a matter of fact, stock time series have their own characteristics over other time series. 
\end{abstract}




\section{Introduction}
The tasks of segmentation and dimensionality reduction, as well as identification of trends, are fundamental to allow a number of time series analysis and mining tasks. As a matter of fact, the fields of application of such procedures are numerous (ECG, exchange rates, sensors detections of any kind, \dots); moreover methods to perform this kind of preprocessing differ greatly, due to the nature and inherent characteristics of data. As a consequence, literature regarding these issues is vast. 

In particular, our focus in this report is on stock market data, which is outstandingly large in size, noisy and continuously updated. This paper is structured as follows: in \cref{sec:theory} we are providing a theoretical introduction and overview on the problem. Then in \cref{sec:methods} a number of research papers about dimensionality reduction and piecewise segmentation are listed and summarised. Then, in \cref{sec:implementations} we are going to present the implementations of a few methods. 



\section{Time series dimensionality reduction}\label{sec:theory}

In relevant literature, many algorithms have been proposed for representing time series into their segmented forms, and many methods for creating time series representations can be found.

Generally, basic classification of algorithmic methods for segmentation implies their division into two great categories: offline, and online algorithms. The essence of offline segmentation is contained in the scanning and division of the entire time series $ T = \left\{ \left( y_1, t_1\right), \left(y_2, t_2 \right), \dots ,\left(y_n, t_n\right)\right\} $ into a number of segments, while the available number of data (data points) remains unchanged. On the other hand, in the online segmentation the uploading of new data points is performed in parallel with the execution of the algorithm. Data points $\left(y_i, t_i\right)$ are added one by one at every time step $t_i$, (for $i = 1,2, \dots ,n$, where $n$ is growing potentially forever); the algorithm must decide whether the new data points belong to the previous segment, or should be assigned to a new segment, which starts at $t_i$. 

Based on the essential context of the offline and online segmentation, numerous algorithms for segmentation of time series are usually classified into one of the following categories of algorithms: \emph{top-down} algorithm; \emph{bottom-up} algorithm; and \emph{sliding window} algorithm. As a result of combining bottom-up and sliding window procedures, a new, precise, streaming \emph{SWAB} algorithm (Sliding Window And Bottom-up algorithm), has been developed.

\paragraph{Top-down} The top-down algorithm, often called \enquote{divide and conquer} or \enquote{binary split}, starts with conditional observing of non-segmented time series as one major segment. Based on the consideration of all possible variants for initial division, the best location for placing the boundary (breaking point) that splits original time series in two segments, $S_1$ (left) and $S_2$ (right), is identified, but in such a way that the difference between those two segments is maximal. Both of these segments are then tested from the aspect of the level of the approximation error. If the approximation error of the observed segment is below the user-defined threshold, the segmentation procedure stops, and the tested segment is accepted. On the other hand, if the approximation error is above the user-defined threshold, further division of the tested segment into two new (sub)segments is performed. For each of the newly formed segments, the process of division into two new segments is repeated in an identical manner, without effect on the location of the breaking point determined in the previous iteration (step). The algorithm repeats these steps until some of the defined stopping criteria is satisfied (i.e., when further division no longer contributes to the minimisation of the segment or segmentation error): 
\begin{enumerate}
	\item $k$ number of segments, and/or,
	\item approximation error less than the user-specified threshold.
\end{enumerate} 

\paragraph{Bottom-up} The bottom-up algorithm, often called \enquote{iterative merge}, as a natural complement to the top-down algorithm, begins by dividing the original time series, of length $n$, into a large number of very small segments with equal lengths. In the next step, based on the comparison of each pair of consecutive segments (including left and right neighbour), the pairs that cause the smallest increase in the error are being identified, and consequently merged in one new, bigger segment. The algorithm repeats these steps until some of the defined stopping criteria is satisfied: 
\begin{enumerate}
	\item $k$ number of segments, and/or,
	\item approximation error greater than specified threshold.
\end{enumerate}

\paragraph{Sliding window} The process of segmentation by using the sliding window algorithm, often called \enquote{brute force} or \enquote{one-pass algorithm}, begins by determining the left boundary (anchor) of the first potential segment (usually the first data point of a time series), which is also the starting point for the window which slides (to the right) along the time series, and in that manner provides identification and selection of segments that satisfy predefined segmentation criterion (user-specified threshold). While sliding down the sequence, the size of the window gradually increases, since all the visited data points on its journey, automatically become potential elements of potential segment, until the error of the potential segment does not become greater than the user-specified threshold. At this point, the right boundary of the moving window ceases to be unknown. In that manner, the length of the certain segment is being determined, and the stopping point of the newly formed segment becomes the new anchor (i.e., the starting point of the next potential segment).

\paragraph{SWAB} SWAB is a more accurate algorithm for segmentation of data streams. Its name indicates that it is the approach that is based on a combination of the sliding window and bottom-up algorithms. SWAB segmentation algorithm begins by defining and selecting one, initial size of the buffer that is big enough to contain enough data to create 5 or 6 segments. In the next step, the bottom-up procedure is applied to the data (data points) inside the buffer. In that way, the leftmost segment is detected, and the data points that correspond to the detected segment are reported and removed from the buffer. Then, in the buffer, using classical sliding window procedure for defining new entry points, new data points are added. It follows re-application of the bottom-up procedure in the buffer. Obviously, the described process of incorporating new data points in the buffer can be repeated as long as the data arrive in continuous streams, potentially indefinitely. 





\section{Methods}\label{sec:methods}
 
What follows is a selection of academical papers focusing specifically on the dimensionality reduction applied to financial time series. 


\newpaper{Fu2011164}{overview}
In this paper, a comprehensive revision on the existing time series data mining research is given. They are generally categorised into representation and indexing, similarity measure, segmentation, visualisation and mining. Moreover state-of-the-art research issues are also highlighted. The primary objective of this paper is to serve as a glossary for interested researchers to have an overall picture on the current time series data mining development and identify their potential research direction to further investigation. 


\newpaper{Yu2006}{PIP, pattern matching} 

Given a set of pattern templates, evolutionary computation is an appropriate tool to segment time series flexibly and effectively. In this paper is proposed a new distance measure based on pattern distance for fitness evaluation. Time sequence is represented by a series of perceptually important points and converted into piecewise trend sequence. Pattern distance measures the trend similarity of two sequences.


\newpaper{Chung:2004:EAP:2221374.2221698}{pattern matching, PIP}

Here is considered the problem of identifying a suitable set of time points for segmenting the time series in accordance with a given set of pattern templates (e.g., a set of technical patterns for stock analysis). The use of fixed-length segmentation is an oversimplified approach to this problem; hence, a dynamic approach (with high controllability) is preferable so that the time series can be segmented flexibly and effectively according to the needs of the users and the applications. It is proposed an evolutionary time series segmentation algorithm that allows a sizeable set of pattern templates to be generated for mining or query. In addition, defining similarity between time series (or time series segments) is of fundamental importance in fitness computation. By identifying the perceptually important points (PIPs) directly from the time domain, time series segments and templates of different lengths can be compared and intuitive pattern matching can be carried out in an effective and efficient manner.


\newpaper{Fu2007347}{pattern matching, PIP}

When it comes to locate the technical patterns in the stock price movement charts to analyse the market behaviour, there are two main problems: how to define those preferred patterns (technical patterns) for query and how to match the defined pattern templates in different resolutions. Defining the similarity between time series (or time series subsequences) is of fundamental importance. By identifying the perceptually important points (PIPs) directly from the time domain, time series and templates of different lengths can be compared. Three ways of distance measure, including Euclidean distance (PIP-ED), perpendicular distance (PIP-PD) and vertical distance (PIP-VD), for PIP identification are compared in this paper. After the PIP identification process, both template- and rule-based pattern-matching approaches are introduced.


\newpaper{Fu2008277}{PIP, binary tree representation}

Stock time series has its own characteristics over other time series. Dimensionality reduction is an essential step before many time series analysis and mining tasks. In this paper, financial time series are represented according to the importance of the data points. With the concept of data point importance, a tree data structure, which supports incremental updating, is proposed to represent the time series and an access method for retrieving the time series data point from the tree, which is according to their order of importance, is introduced. This technique is capable to present the time series in different levels of detail and facilitate multi-resolution dimensionality reduction of the time series data.


\newpaper{5250725}{PIP, binary tree representation}

Financial time series often exhibit high degrees of fluctuation which are considered as noise in time series analysis. To remove noise, in financial time series analysis, it is important to retain the important points and remove others. Here is proposed the Zigzag based Perceptually Important Point Identification method to collect those zigzag movement important points. Further, it is also proposed Zigzag based Multiway Search Tree to index these important points.


\newpaper{clinverse}{pattern matching, PIP}

In this paper, according to the characteristics of PTSS (Patterned-based time series segmentation), a generalized model is proposed for PTSS. Aa new interpretation for PTSS is given by comparing this problem with the prototype-based clustering (PC). Then, a novel model, called clustering-inverse model (CI-model), is presented. Finally, two algorithms are presented to implement this model.


\newpaper{5961935}{turning points, PIP}

In this paper, a novel time series segmentation method based on Turning Points is proposed. Turning Points are extracted from the maximum or minimum points of the time series. The proposed segmentation method generates segments at different levels of details and achieves satisfactory results in preserving higher number of trends compared to an existing segmentation approach.


\newpaper{Vo2013}{turning points}

In this paper it is proposed an approach established on turning points to reduce the dimensions of stream time series data, and this task supports the prediction process faster in in stream data environment. The turning points are extracted from the maximum or minimum points of the time series data proved more efficient and effective in the process of processing data for time series predictive analysis.


\newpaper{Fu20081217}{PIP, binary tree representation}

In this paper, a framework that represents and visualises time series data based on data point importance is proposed. Furthermore, discovering frequently appearing and surprising patterns are non-trivial tasks in financial applications. A method for discovering patterns across different resolutions is proposed. The proposed method is based on a modified version of VizTree. By converting the time series to symbol string based on data point importance, the potential patterns with different lengths can be encoded in the VizTree for visual pattern discovery while the important points and the overall shape of the time series patterns can be preserved even under a high compression ratio.









\section{Implementations}\label{sec:implementations}

\paragraph{Turning points} Due to its simplicity, the apparently good results and openness to further improvements, we decided to implement the \emph{turning points} method presented in \cite{5961935}. So, in the following \cref{sec:tp_matlab,sec:tp_python} we are presenting our implementation both in \matlab and in Python of the method.

Basically, the algorithm is divided into two phases. First, during a preprocessing phase all points which are neither local maxima nor minima are discarded; then some patterns are simplified, since immaterial. However, we are not going to discuss the theoretical foundations and the steps of the algorithm in detail, as the aforementioned article if sufficient.

\paragraph{Other implementations} In order to provide a better view on the implementations, we are presenting other methods in \cref{sec:other}.



\subsection{Turning points in \matlab}\label{sec:tp_matlab}

\paragraph{Implementation} The method is developed and tested over \csv files downloaded from Yahoo! Finance website\footnote{\url{http://finance.yahoo.com/market-overview/}}. Should another source be used, basic adaptations may be necessary, mostly in the handling of temporal data\footnote{Due to our limited knowledge of Matlab language, we may have dealt with the source data in a na{\"i}ve way. Nevertheless, the procedure seems to work well.} (i.e., dates), which is performed in \texttt{TP\_prepareData()} procedure. 

After importing the aforementioned \csv file (we suggest to use the graphical interface provided by \matlab itself), the user should issue the following command, in order to compute and plot the results: \mint{matlab}{y = TurningPoints(time, values, n);}

Here, \texttt{time} and \texttt{values} represent the time series as imported from the \csv; \texttt{n} instead lets the user specify the number of times the algorithm shall be performed (preprocessing is excluded from this count). The results are both displayed in a plot and stored in \texttt{y} variable.

\texttt{TurningPoints()} function is implemented as shown in \cref{lst:turningPoints}, with the support of some side functions.

\clearpage % TODO Clearpage

\begin{listing}%[H]
 
\inputminted[firstline = 1, lastline = 50]{matlab}{../code/TurningPoints.m}

\caption{\texttt{TurningPoints()} function.}\label{lst:turningPoints}

\end{listing}

\clearpage % TODO Clearpage

The main supporting function is \texttt{TP\_preprocess()}, which implements the preprocessing phase, and is presented in \cref{lst:preprocess}. The other supporting functions are of secondary importance, so we are not presenting them here. Basically we have: \begin{itemize}
	\item \texttt{TP\_prepareData()} takes in input raw Yahoo! Finance data and prepares them to the processing;
	\item \texttt{TP\_cleaning()} cleans data matrix after processing;
	\item \texttt{TP\_output()} shows information about the processing (i.e., the number of deleted elements).
\end{itemize} 

\begin{listing}%[H]
 
\inputminted[firstline = 53, lastline = 78]{matlab}{../code/TurningPoints.m}

\caption{\texttt{TP\_preprocess()} supporting function.}\label{lst:preprocess}

\end{listing}


\paragraph{Test} To conclude our discussion, we are presenting some tests we performed. Weekly stock market data from A2A (\texttt{A2A.MI}) over the whole 2015 were used as source. In particular, we plotted the weekly \texttt{Close} time series. In \cref{fig:a2a_w_2015} we show the original data in blue, the preprocessed data in orange and the data after one full run of the algorithm in yellow. 

Original data contains 53 samples; preprocessing reduces them to 27, and finally, after the actual processing, only 12 samples are left. 

\clearpage % TODO Clearpage

\begin{figure}%[H]
	
	\makebox[\textwidth][c]{\includegraphics[width=1.8\textwidth]{img/a2a_W_15}}
	
	\caption{\texttt{A2A.MI} weekly 2015. Original data is in the correct position. The other two series are shifted down by 0.1 each.}\label{fig:a2a_w_2015}

\end{figure}


\subsection{Turning points in Python}\label{sec:tp_python}

Python is an excellent language for working with financial data.There is a strong support for downloading, manipulating, and visualizing financial market data through popular open source libraries like Pandas and Matplotlib.Sources:-Yahoo! Finance-Google Finance-St.Louis FED (FRED)-Kenneth French’s data library-World Bank-Google Analytics    Possible indicators for Yahoo Finance:-Open         -High-Low-Close-Volume-Adj CloseThe inputs of the program are: -period of time to be considered-name of the share-level of deepnessThe program che be divided in four parts:1) prepare, cleaning and convert the data in the time serie2) find the turning points.Local maximum and minimum points are often called Turning Points (TPs) since they indicate the change in the trend of the stock during a period. 3) flatten the time serie at different level of deepness4) plot the results The algorithm presented in the paper has been slightly modified. In the conditions for finding the TPs: -greater/less- than has been replaced with -greater or equal/less or equal than-, and the points at the same level have been adjusted afterwards.     This modification brought to a better alignment between the original data and the trend function.


\begin{listing}%[H]
 
\inputminted[firstline = 2, lastline = 47]{python}{../code/TPs_Python.py}

\caption{Python implementation (1).}\label{lst:python}

\end{listing}
 
 
\begin{listing}%[H]
 
\inputminted[firstline = 50, lastline = 90, breaklines = true]{python}{../code/TPs_Python.py}

\caption{Python implementation (2).}\label{lst:python2}

\end{listing}
 


\begin{figure}%[H]
	
	%\makebox[\textwidth][c]{\includegraphics[trim = {0 0 0 12.7cm}, clip,  width=1.5\textwidth]{img/L}}
	
	\makebox[\textwidth][c]{\includegraphics[trim = {0 0 0 12.3cm}, clip,  width=1.5\textwidth]{img/LTP}}
	
	\makebox[\textwidth][c]{\includegraphics[trim = {0 0 0 12.3cm}, clip,  width=1.5\textwidth]{img/L1}}
	
	\makebox[\textwidth][c]{\includegraphics[trim = {0 0 0 12.3cm}, clip,  width=1.5\textwidth]{img/L2}}
	
	\makebox[\textwidth][c]{\includegraphics[trim = {0 0 0 12.3cm}, clip,  width=1.5\textwidth]{img/L5}}
	
	\caption{\texttt{A2A.MI} daily \texttt{Close} quotations from mid-april 2015 to mid-april 2016 after preprocessing (local maxima/minima), then after one, two and five iterations of the algorithm.}\label{fig:python}

\end{figure}


\subsection{Other implementations}\label{sec:other}
\lipsum[4-6] %TODO Ramer-Douglas-Peucker





\clearpage
\defbibnote{prenote}{DOI (Digital Object Identifier), when defined, serves as URL to retrieve the document. Documents may be accessible only on institutional log in (e.g., academic credentials).}
%\nocite{*}
\printbibliography[title={Bibliography}, prenote=prenote] 




\end{document}







